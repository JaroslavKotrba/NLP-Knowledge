# GLOSSARY

# TOKENIZING
# Tokenizing the text (breaking it into smaller units like words or subwords), and converting it into a format suitable for training.
# Introduced by Vaswani et al. in 2017, the Transformer architecture is the backbone of most modern LLMs. It consists of layers of attention mechanisms and feed-forward neural networks.

# EMBEDDINGS
# Embeddings transform the query text (tokens) into a numerical vector that captures the semantic meaning of the text.

# SELF-SUPERVISED LEARNING
# Self-supervised learning is a form of unsupervised learning where the model learns to predict part of the input from other parts of the input. This method allows the model to learn useful representations and patterns from the data without requiring manually labeled datasets.

# RETRIEVAL-AUGMENTED GENERATON (RAG)
# Retrieval-Augmented Generation (RAG) is a hybrid approach that enhances the capabilities of large language models (LLMs) by combining information retrieval with text generation.
# This method leverages the strengths of both retrieval systems and generative models to produce more accurate and contextually relevant responses.

# FINE-TUNING
# Fine-Tuning: Modifies the internal weights of the model to adapt it to a specific task. It requires labeled task-specific data for training.
# RAG: Enhances generation by retrieving relevant information from external sources at inference time. It uses retrieval to augment the generation process without changing the modelâ€™s internal weights.
